<!DOCTYPE html>
<html lang="en">

<head>
    <title>L2ID</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


    <link href="https://fonts.googleapis.com/css?family=B612+Mono|Cabin:400,700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="fonts/icomoon/style.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
          integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/jquery.fancybox.min.css">

    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">

    <link rel="stylesheet" href="css/aos.css">
    <link href="css/jquery.mb.YTPlayer.min.css" media="all" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="css/style.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-88572407-1', 'auto');
        ga('send', 'pageview');
    </script>
</head>

<body data-offset="300" data-spy="scroll" data-target=".site-navbar-target">

    <div class="site-wrap">

        <div class="site-mobile-menu site-navbar-target">
            <div class="site-mobile-menu-header">
                <div class="site-mobile-menu-close mt-3">
                    <span class="icon-close2 js-menu-toggle"></span>
                </div>
            </div>
            <div class="site-mobile-menu-body"></div>
        </div>
    
        <div class="header-top">
            <div class="container">
                <div class="row align-items-center">
                    <div class="col-12 col-lg-6 d-flex">
                        <a href="index.html" class="site-logo">
                            Learning from Limited and Imperfect Data (L2ID): Localization Challenges
                        </a>
                        <a href="#"
                           class="ml-auto d-inline-block d-lg-none site-menu-toggle js-menu-toggle text-black"><span
                                class="icon-menu h3"></span></a>
    
                    </div>
                    <div class="col-12 col-lg-6 ml-auto d-flex">
                        <div class="ml-md-auto top-social d-none d-lg-inline-block">
                            <a href="#" class="d-inline-block p-3"> </a>
                            <a href="#" class="d-inline-block p-3"> </a>
                            <a href="#" class="d-inline-block p-3"> </a>
                        </div>
    
                    </div>
                    <!--          <div class="col-6 d-block d-lg-none text-right">-->
    
                </div>
            </div>
        </div>
        <div class="site-navbar py-2 js-sticky-header site-navbar-target d-none pl-0 d-lg-block" role="banner">
    
            <div class="container">
                <div class="d-flex align-items-center">
    
                    <div class="mr-auto">
                        <nav class="site-navigation position-relative text-right" role="navigation">
                            <ul class="site-menu main-menu js-clone-nav mr-auto d-none pl-0 d-lg-block">
                                <li class="active">
                                    <a href="index.html" class="nav-link text-left">Home</a>
                                </li>
                                <li>
                                    <a href="index.html#dates" class="nav-link text-left">Important dates</a>
                                </li>
                                <li>
                                    <a href="index.html#schedule" class="nav-link text-left">Schedule</a>
                                </li>
                                <li>
                                    <a href="index.html#speakers" class="nav-link text-left">Speakers</a>
                                </li>
                                <li class="nav-item dropdown">
                                    <a class="nav-link dropdown-toggle" id="navbarDropdown"
                                       role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Challenges
                                    </a>
                                    <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                        <a class="dropdown-item" href="challenge_localization.html">Localization</a>
                                        <a class="dropdown-item" href="challenge_classification.html">Classification</a>
                                    </div>
                                </li>
                                <li>
                                    <a href="index.html#people" class="nav-link text-left">Organizers</a>
                                </li>
                                <!--               <li class="nav-item dropdown">
                                                  <a class="nav-link dropdown-toggle" href="challenge.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                       Challenge
                                     </a>
                                              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                               <a class="dropdown-item" href="challenge.html#challenge1">Object semantic segmentation with image-level supervision</a>
                                               <a class="dropdown-item" href="challenge.html#challenge2">Scene parsing with point-based supervision</a>
                                             </div>
                                             </li> -->
                                <li class="nav-item dropdown">
                                    <a class="nav-link dropdown-toggle" id="navbarDropdown"
                                       role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Previous
                                    </a>
                                    <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                        <a class="dropdown-item" href="https://lidchallenge.github.io/LID2019/">LID 2019</a>
                                        <a class="dropdown-item" href="https://lidchallenge.github.io/">LID 2020</a>
                                        <a class="dropdown-item" href="https://www.learning-with-limited-labels.com"> L3V</a>
                                    </div>
                                </li>
                            </ul>
                        </nav>
    
                    </div>
    
                </div>
            </div>
    
        </div>
    
    </div>


<div class="site-section">
    <div class="container">
        <div class="row">
            <!-- Challenge 1-->
            <div class="col-lg-12" id="challenge1">

                <!-- <p style="text-align: justify">We will organize the second Learning from Imperfect Data (LID) challenge on object semantic segmentation and scene parsing, which includes two competition tracks.  </p>  （<strong>challenge deadline: June 8, 2019</strong>） -->
                <p>In conjunction with this workshop, we will hold three challenges this
                    year.</p>  <!--（<strong>challenge deadline: June 8, 2019</strong>）-->

                <div class="section-title">
                    <h1>Track1</h1>
                    <h4>Weakly-supervised Semantic Segmentation</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p>
                            This track targets on learning to perform object semantic segmentation using image-level
                            annotations as supervision [1, 2, 3]. The dataset is built upon the image detection track of
                            ImageNet Large Scale Visual Recognition Competition (ILSVRC) [4], which totally includes
                            456, 567 training images from 200 categories. We provide pixel-level annotations of 15K
                            images (validation/testing: 5, 000/10, 000) for evaluation.
                        </p>
                        <!-- </br> -->

                        <ul>
                            <!-- <li> For training, all the images in the training set of ILSVRC DET are permitted. </li> -->
                            <!-- <li> If supervised saliency detection is applied, only MSRA-B dataset is permitted.  </li> -->
                            <li><strong>Evalution:</strong> Mean Intersection-Over-Union (IoU) score over 200 categories.</li>
                            <li><strong>Download: </strong> The training dataset is available at <a
                                    href="http://image-net.org/image/ILSVRC2017/ILSVRC2017_DET.tar.gz">Imagenet DET</a> <a
                                        href="https://pan.baidu.com/s/1YoHmBqXofudQbGMOzRVW1Q">Baidu Drive (pwd: 5e7g) </a> 
                                <a href="https://drive.google.com/file/d/1Gj0TtsQ7tfaH94jBRcFdRxQs_MBH0pXt/view?usp=sharing">Google Drive</a>
                                , val and test dataset are available at <a
                                        href="https://pan.baidu.com/s/1_rzQNkTEFmTJdiYYbhySSQ">Baidu Drive </a> and <a
                                        href="https://drive.google.com/open?id=1B0enLzxyIULbRZWUi0XpNnXCu7nwFI-f">
                                    Google Drive</a> <br/>
                                <strong class="text-danger">Note: </strong> The image label information can be extracted using the <a href="https://drive.google.com/open?id=1ajioybXZYPIXUyQl7G4MRykr7AvelAL6">scripts</a> </li>
                            <li><strong>Submission: </strong> <a
                                    href="https://evalai.cloudcv.org/web/challenges/challenge-page/556/overview">https://evalai.cloudcv.org/web/challenges/challenge-page/556/overview</a>
                            </li>
                        </ul>
                        <table align="center" class="table  table-hover">
                            <thead>
                            <tr>

                                <th scope="col"> Rank </th>
                                <th scope="col"> Participant team </th>
                                <th scope="col"> Mean IoU </th>
                                <th scope="col"> Mean accuracy </th>
                                <th scope="col"> Pixel accuracy </th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td>1st</td>
                                <td>Shuo Li, Zehua Hao, Yaoyang Du, Fang Liu<sup>#</sup>, Licheng Jiao<sup>#</sup>. Xidian University, IPIU Lab 
                                    [<a href="https://drive.google.com/file/d/1BykFWzZGq4nz4sQqvPJ3V1lT5OLkogfO/view?usp=sharing">slide</a>]
                                </td>
                                <td>49.06</td>
                                <td>68.1</td>
                                <td>86.64</td>
                            </tr>
                            <tr>
                                <td>2nd</td>
                                <td>Junwen Pan, Yongjuan Ma and Pengfei Zhu. Tianjin University 
                                    [<a href="https://drive.google.com/file/d/1v7izuzmsob_i5ozeg4K6Pa5Sj7nTmYTl/view?usp=sharing">slide</a>]
                                </td>
                                <td>49.03</td>
                                <td>67.87</td>
                                <td>87.53</td>
                            </tr>
                            <tr>
                                <td>3rd</td>
                                <td>Xun Feng, Zhenyuan Chen<sup>1</sup>, Zhendong Wang<sup>1</sup>, Yibing Zhan<sup>2</sup>, Chen Gong<sup>1</sup>. 
                                   <sup>1</sup>Nanjing University of Science and Technology, <sup>2</sup>JD Explore Academy, JD.com 
                                    [<a href="https://drive.google.com/file/d/1-0W-uYAXwArG-zgOgz6YiyL_0y3FLDpr/view?usp=sharing">slide</a>]
                                </td>
                                <td>39.68</td>
                                <td>53</td>
                                <td>82.18</td>
                            </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>

            <!-- Challenge 2-->

            <div class="col-lg-12" id="challenge2">
                <div class="section-title">
                    <h1>Track2</h1>
                    <h4> Weakly supervised product retrieval</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <!-- <p> Beyond object segmentation, background categories such as wall, road, sky need to be further specified for the scene parsing, which is a challenging task compared with object semantic segmentation. Thus, it will be more difficult and expensive to manually annotate pixel-level mask for this task. In this track, we propose to leverage several labeled points that are much easier to obtain to guide the training process.
      The dataset is built upon the well-known ADE20K, which includes 20,210 training images from 150 categories. We provide the point-based annotations on the training set. Please download the data from <a href="http://sceneparsing.csail.mit.edu/data/LID2019"> LID Challenge Track2 data </a>. <br/> -->
                        <p> Given a photo containing multiple product instances and a user-provided description, the track aims to detect the boxes of each product and retrieve the correct single product image in the gallery. We collect 1132830 real-world product photos in e-commerce website where each photo contains 2.83 products on average and corresponds to a user-provided description, and a single-product gallery with 40033 images for evaluating the retrieval performance. We split 9220 photos and their corresponding descriptions as the test set and provide product-level bounding boxes for each photo. This new track poses a very common setting in real-world application (e.g. e-commerce) and an interesting testbed for learning from imperfect data which testifies both the weak-supervised object retrieval given a caption, fine-grained instance recognitions and cross-modality (i.e. text and image) object-level retrieval. More details of this challenge are provided at <a href="https://competitions.codalab.org/competitions/30123">https://competitions.codalab.org/competitions/30123</a> 
                        </p>
                        <!-- </br> -->
                        <!-- <strong> <span class="text-danger">*Note:</span> </strong> -->
                        <!-- </p> -->
                       <table align="center" class="table  table-hover">
                            <thead>
                            <tr>

                                <th scope="col"> Rank </th>
                                <th scope="col"> Participant team </th> 
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td>1st</td>
                                <td>Baojun Li, Gengxin Wang, Jiamian Huang, Tao Liu, Zhiwei Shi, Zhimeng Wang. Joyy Al Research
                                    [<a href="https://drive.google.com/file/d/1M5sWKick4h035paTcnhTB_bJF0Nl7YWV/view?usp=sharing">slide</a>]
                                
                                </td>
                 
                            </tr>
                            <tr>
                                <td>2nd</td>
                                <td>Yanxin Long, Shuai Lin. Sun Yat-sen University </td>
                            </tr> 
                            <tr>
                                <td>3rd</td>
                                <td>Hanyu Zhang, Pengliang Sun, Xing Liu. Chinese University of Hong Kong </td>
                            </tr> 
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>

            <!-- Challenge 3-->
            <div class="col-lg-12" id="challenge3">
                <div class="section-title">
                    <h1>Track3</h1>
                    <h4> Weakly-supervised Object Localization</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p> This track targets on making the classification networks be equipped with the ability of
                            object localization [7, 8, 9]. The dataset is built upon the image
                            classification/localization track of ImageNet Large Scale Visual Recognition Competition
                            (ILSVRC), which totally includes 1.2 million training images from 1000 categories. We
                            provide pixel-level annotations of 44, 271 images (validation/testing: 23, 151/21, 120) for
                            evaluation.
                        </p>
                        <ul>
                            <li><Strong>Evalution:</Strong> IoU curve. With the predicted object localization map, we calculate
                                the IoU scores between the foreground pixels and the ground-truth masks under different
                                thresholds. In the ideal curve, the highest IoU score is expected to close to 1.0. The
                                threshold value corresponding to the highest IoU score is expected to be 255 since the
                                higher threshold values can reflect a higher contrast between the target object and the
                                background.
                            </li>
                            <li> <Strong>Download: </Strong> validation dataset, test list and evaluation scripts are available at <a
                                        href="https://pan.baidu.com/s/1Ob7bzJcvirpkqZ-gQL-MjA">Baidu Drive (pwd: z5yp) </a> and <a
                                        href="https://drive.google.com/drive/folders/1rd3iV9Xif2tRgofQWrL3qH1_lIFkicdI?usp=sharing">
                                    Google Drive</a></li> </li>
                            <li> <Strong>Submission: </Strong> <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/557/overview">https://evalai.cloudcv.org/web/challenges/challenge-page/557/overview</a> </li>
                            <li> <strong class="text-danger"> The evaluation server error occurred. Please send the zipped results to liutingtianna@gmail.com for evaluation. </strong> </li>
                         </ul>
                         <table align="center" class="table  table-hover">
                            <thead>
                            <tr>

                                <th scope="col"> Rank </th>
                                <th scope="col"> Participant team </th>
                                <th scope="col"> Peak_IoU </th>
                                <th scope="col"> Peak_Threshold </th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td>1st</td>
                                <td>Xun Feng,Zhenyuan Chen <sup>1</sup>, Zhendong Wang <sup>1</sup>, Yibing Zhan <sup>2</sup>, Chen Gong<sup>1</sup>.
                                   <sup>1</sup>Nanjing University of Science and Technology, <sup>2</sup>JD Explore Academy, JD.com 
                                    [<a href="https://drive.google.com/file/d/1-EoiJv45Q3XS29tmK-4942ZZpYxGrTtk/view?usp=sharing">slide</a>]
                                </td>
                                <td>0.697</td>
                                <td>149</td>
                            </tr>
                            <tr>
                                <td>2nd</td>
                                <td>Yonsei-CVPR</td>
                                <td>0.55</td>
                                <td>41</td>
                            </tr> 
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
            <!-- Challenge 4-->
            <div class="col-lg-12" id="challenge3">
                <div class="section-title">
                    <h1>Track4</h1>
                    <h4> High-resolution Human Parsing</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p> This track aims to recognize human parts (19 semantics in total) within high-resolution images by learning with low-resolution ones, which is few explored before. To this end, we annotated 10,500 single-person images (training/validation/testing: 6,000/500/4,000) with an average resolution of 3950 by 2200. Besides the provided high-resolution images, off-the-shelf low-resolution datasets such as LIP and Pascal-Person-Part are welcome adopted for pre-training. This new track poses a new task of learning from imperfect data, transferring the learned knowledge from low-resolution images to high-resolution images. More details of this challenge are provided at <a href="https://competitions.codalab.org/competitions/30375">https://competitions.codalab.org/competitions/30375</a>
                        </p>
                        <ul>
                            Details will be available soon.
                        </ul>
                        <table align="center" class="table  table-hover">
                            <thead>
                            <tr>

                                <th scope="col"> Rank </th>
                                <th scope="col"> Participant team </th>
                                <th scope="col"> eIoU </th>
                                <th scope="col"> mIoU</th> 
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td>1st</td>
                                <td>Lu Yang <sup>1</sup>, Liulei Li<sup>2, 4</sup>, Tianfei Zhou<sup>3</sup>, Wenguan Wang<sup>3</sup>, Yi Liu<sup>4</sup>, Qing Song<sup>1</sup>. <sup>1</sup>BUPT-PRIV, <sup>2</sup>BIT, <sup>3</sup>ETH Zurich, <sup>4</sup>Baidu
                                [<a href="https://drive.google.com/file/d/1wxtbcsCHtq5Vh96DGpQk-OrIKEwTykrS/view?usp=sharing">slide</a>]
                                </td>
                                <td>48.29</td>
                                <td>79.32</td> 
                            </tr>
                            <tr>
                                <td>2nd</td>
                                <td>DeepBlueAI team [<a href="https://drive.google.com/file/d/1US0sfUdVG1tuAL3GdP98TNpc_LEsJNi_/view?usp=sharing">slide</a>]</td>
                                <td>46.24</td>
                                <td>77.49</td> 
                            </tr>
                            <tr>
                                <td>3rd</td>
                                <td>DISL</td>
                                <td>43.87</td>
                                <td>76.72</td> 
                            </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>

            



    </div>

    <div class="col-lg-12" id="challenge2">
        <div style="display:inline-block;width:500px;">
            <script async="async" src="//rc.rev
            olvermaps.com/0/0/7.js?i=2hlmeh3dic1&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;br=19&amp;sx=0"
                    type="text/javascript"></script>
        </div>
    </div>
</div>
</div>
<!-- END section -->


<div class="footer">
    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="copyright">
                    <p>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                        Copyright &copy;<script>document.write(new Date().getFullYear());</script>
                        All rights reserved | This template is made with <i aria-hidden="true"
                                                                            class="icon-heart text-danger"></i> by
                        <a href="https://colorlib.com" target="_blank">Colorlib</a>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>


</div>
<!-- .site-wrap -->


<!-- loader -->
<div class="show fullscreen" id="loader">
    <svg class="circular" height="48px" width="48px">
        <circle class="path-bg" cx="24" cy="24" fill="none" r="22" stroke="#eeeeee" stroke-width="4"/>
        <circle class="path" cx="24" cy="24" fill="none" r="22" stroke="#ff5e15" stroke-miterlimit="10"
                stroke-width="4"/>
    </svg>
</div>

<script src="js/jquery-3.3.1.min.js"></script>
<script src="js/jquery-migrate-3.0.1.min.js"></script>
<script src="js/jquery-ui.js"></script>
<script src="js/popper.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/owl.carousel.min.js"></script>
<script src="js/jquery.stellar.min.js"></script>
<script src="js/jquery.countdown.min.js"></script>
<script src="js/bootstrap-datepicker.min.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/aos.js"></script>
<script src="js/jquery.fancybox.min.js"></script>
<script src="js/jquery.sticky.js"></script>
<script src="js/jquery.mb.YTPlayer.min.js"></script>


<script src="js/main.js"></script>


</body>

</html>

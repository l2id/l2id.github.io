<!DOCTYPE html>
<html lang="en">

<head>
    <title>L2ID</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


    <link href="https://fonts.googleapis.com/css?family=B612+Mono|Cabin:400,700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="fonts/icomoon/style.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
          integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/jquery.fancybox.min.css">

    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">

    <link rel="stylesheet" href="css/aos.css">
    <link href="css/jquery.mb.YTPlayer.min.css" media="all" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="css/style.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-88572407-1', 'auto');
        ga('send', 'pageview');
    </script>
</head>

<body data-offset="300" data-spy="scroll" data-target=".site-navbar-target">

    <div class="site-wrap">

        <div class="site-mobile-menu site-navbar-target">
            <div class="site-mobile-menu-header">
                <div class="site-mobile-menu-close mt-3">
                    <span class="icon-close2 js-menu-toggle"></span>
                </div>
            </div>
            <div class="site-mobile-menu-body"></div>
        </div>
    
        <div class="header-top">
            <div class="container">
                <div class="row align-items-center">
                    <div class="col-12 col-lg-6 d-flex">
                        <a href="index.html" class="site-logo">
                            Learning from Limited and Imperfect Data (L2ID)
                        </a>
                        <a href="#"
                           class="ml-auto d-inline-block d-lg-none site-menu-toggle js-menu-toggle text-black"><span
                                class="icon-menu h3"></span></a>
    
                    </div>
                    <div class="col-12 col-lg-6 ml-auto d-flex">
                        <div class="ml-md-auto top-social d-none d-lg-inline-block">
                            <a href="#" class="d-inline-block p-3"> </a>
                            <a href="#" class="d-inline-block p-3"> </a>
                            <a href="#" class="d-inline-block p-3"> </a>
                        </div>
    
                    </div>
                    <!--          <div class="col-6 d-block d-lg-none text-right">-->
    
                </div>
            </div>
        </div>
        <div class="site-navbar py-2 js-sticky-header site-navbar-target d-none pl-0 d-lg-block" role="banner">
    
            <div class="container">
                <div class="d-flex align-items-center">
    
                    <div class="mr-auto">
                        <nav class="site-navigation position-relative text-right" role="navigation">
                            <ul class="site-menu main-menu js-clone-nav mr-auto d-none pl-0 d-lg-block">
                                <li class="active">
                                    <a href="index.html" class="nav-link text-left">Home</a>
                                </li>
                                <li>
                                    <a href="index.html#dates" class="nav-link text-left">Important dates</a>
                                </li>
                                <li>
                                    <a href="index.html#schedule" class="nav-link text-left">Schedule</a>
                                </li>
                                <li>
                                    <a href="index.html#speakers" class="nav-link text-left">Speakers</a>
                                </li>
                                <li class="nav-item dropdown">
                                    <a class="nav-link dropdown-toggle" id="navbarDropdown"
                                       role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Challenges
                                    </a>
                                    <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                        <a class="dropdown-item" href="challenge_localization.html">Localization</a>
                                        <a class="dropdown-item" href="challenge_classification.html">Classification</a>
                                    </div>
                                </li>
                                <li>
                                    <a href="index.html#people" class="nav-link text-left">Organizers</a>
                                </li>
                                <!--               <li class="nav-item dropdown">
                                                  <a class="nav-link dropdown-toggle" href="challenge.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                       Challenge
                                     </a>
                                              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                               <a class="dropdown-item" href="challenge.html#challenge1">Object semantic segmentation with image-level supervision</a>
                                               <a class="dropdown-item" href="challenge.html#challenge2">Scene parsing with point-based supervision</a>
                                             </div>
                                             </li> -->
                                <li class="nav-item dropdown">
                                    <a class="nav-link dropdown-toggle" id="navbarDropdown"
                                       role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Previous
                                    </a>
                                    <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                        <a class="dropdown-item" href="https://lidchallenge.github.io/LID2019/">LID 2019</a>
                                        <a class="dropdown-item" href="https://lidchallenge.github.io/">LID 2020</a>
                                        <a class="dropdown-item" href="https://www.learning-with-limited-labels.com"> L3V</a>
                                    </div>
                                </li>
                            </ul>
                        </nav>
    
                    </div>
    
                </div>
            </div>
    
        </div>
    
    </div>


<div class="site-section">
    <div class="container">
        <div class="row">
            <!-- Challenge 1-->
            <div class="col-lg-12" id="challenge1">

                <!-- <p style="text-align: justify">We will organize the second Learning from Imperfect Data (LID) challenge on object semantic segmentation and scene parsing, which includes two competition tracks.  </p>  （<strong>challenge deadline: June 8, 2019</strong>） -->
                <p>In conjunction with this workshop, we will hold three challenges this
                    year.</p>  <!--（<strong>challenge deadline: June 8, 2019</strong>）-->

                <div class="section-title">
                    <h1>Track1</h1>
                    <h4>Weakly-supervised Semantic Segmentation</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p>
                            This track targets on learning to perform object semantic segmentation using image-level
                            annotations as supervision [1, 2, 3]. The dataset is built upon the image detection track of
                            ImageNet Large Scale Visual Recognition Competition (ILSVRC) [4], which totally includes
                            456, 567 training images from 200 categories. We provide pixel-level annotations of 15K
                            images (validation/testing: 5, 000/10, 000) for evaluation.
                        </p>
                        <!-- </br> -->

                        <ul>
                            <!-- <li> For training, all the images in the training set of ILSVRC DET are permitted. </li> -->
                            <!-- <li> If supervised saliency detection is applied, only MSRA-B dataset is permitted.  </li> -->
                            <li><strong>Evalution:</strong> Mean Intersection-Over-Union (IoU) score over 200 categories.</li>
                            <li><strong>Download: </strong> The training dataset is available at <a
                                    href="http://image-net.org/image/ILSVRC2017/ILSVRC2017_DET.tar.gz">Imagenet DET</a>
                                , val and test dataset are available at <a
                                        href="https://pan.baidu.com/s/1_rzQNkTEFmTJdiYYbhySSQ">Baidu Drive </a> and <a
                                        href="https://drive.google.com/open?id=1B0enLzxyIULbRZWUi0XpNnXCu7nwFI-f">
                                    Google Drive</a> <br/>
                                <strong class="text-danger">Note: </strong> The image label information can be extracted using the <a href="https://drive.google.com/open?id=1ajioybXZYPIXUyQl7G4MRykr7AvelAL6">scripts</a> </li>
                            <li><strong>Submission: </strong> <a
                                    href="https://evalai.cloudcv.org/web/challenges/challenge-page/556/overview">https://evalai.cloudcv.org/web/challenges/challenge-page/556/overview</a>
                            </li>
                        </ul>

                    </div>
                </div>
            </div>

            <!-- Challenge 2-->

            <div class="col-lg-12" id="challenge2">
                <div class="section-title">
                    <h1>Track2</h1>
                    <h4> Weakly supervised product detection and retrieval</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <!-- <p> Beyond object segmentation, background categories such as wall, road, sky need to be further specified for the scene parsing, which is a challenging task compared with object semantic segmentation. Thus, it will be more difficult and expensive to manually annotate pixel-level mask for this task. In this track, we propose to leverage several labeled points that are much easier to obtain to guide the training process.
      The dataset is built upon the well-known ADE20K, which includes 20,210 training images from 150 categories. We provide the point-based annotations on the training set. Please download the data from <a href="http://sceneparsing.csail.mit.edu/data/LID2019"> LID Challenge Track2 data </a>. <br/> -->
                        <p> Given a photo containing multiple product instances and a user-provided description, the track aims to detect the boxes of each product and retrieve the correct single product image in the gallery. We collect 200k real-world product photos in e-commerce website where each photo contains 4 products on average and corresponds to a user-provided description, and a single-product gallery with 500k images for evaluating the retrieval performance. We split 10k photos and their corresponding descriptions as the test set and provide product-level bounding boxes for each photo. This new track poses a very common setting in real-world application (e.g. e-commerce) and an interesting testbed for learning from imperfect data which testifies both the weak-supervised object retrieval given a caption, fine-grained instance recognitions and cross-modality (i.e. text and image) object-level retrieval.
                        </p>
                        <!-- </br> -->
                        <!-- <strong> <span class="text-danger">*Note:</span> </strong> -->
                        <!-- </p> -->
                        <ul>
                            Details will be available soon.
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Challenge 3-->
            <div class="col-lg-12" id="challenge3">
                <div class="section-title">
                    <h1>Track3</h1>
                    <h4> Weakly-supervised Object Localization</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p> This track targets on making the classification networks be equipped with the ability of
                            object localization [7, 8, 9]. The dataset is built upon the image
                            classification/localization track of ImageNet Large Scale Visual Recognition Competition
                            (ILSVRC), which totally includes 1.2 million training images from 1000 categories. We
                            provide pixel-level annotations of 44, 271 images (validation/testing: 23, 151/21, 120) for
                            evaluation.
                        </p>
                        <ul>
                            <li><Strong>Evalution:</Strong> IoU curve. With the predicted object localization map, we calculate
                                the IoU scores between the foreground pixels and the ground-truth masks under different
                                thresholds. In the ideal curve, the highest IoU score is expected to close to 1.0. The
                                threshold value corresponding to the highest IoU score is expected to be 255 since the
                                higher threshold values can reflect a higher contrast between the target object and the
                                background.
                            </li>
                            <li> <Strong>Download: </Strong> validation dataset, test list and evaluation scripts are available at <a
                                        href="https://pan.baidu.com/s/1Ob7bzJcvirpkqZ-gQL-MjA">Baidu Drive (pwd: z5yp) </a> and <a
                                        href="https://drive.google.com/drive/folders/1rd3iV9Xif2tRgofQWrL3qH1_lIFkicdI?usp=sharing">
                                    Google Drive</a></li> </li>
                            <li> <Strong>Submission: </Strong> <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/557/overview">https://evalai.cloudcv.org/web/challenges/challenge-page/557/overview</a> </li>
                    </div>
                </div>
            </div>
            <!-- Challenge 4-->
            <div class="col-lg-12" id="challenge3">
                <div class="section-title">
                    <h1>Track4</h1>
                    <h4> High-resolution Human Parsing</h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p> This track aims to recognize human parts (19 semantics in total) within high-resolution images by learning with low-resolution ones, which is few explored before. To this end, we annotated 10,500 single-person images (training/validation/testing: 6,000/500/4,000) with an average resolution of 3950 by 2200. Besides the provided high-resolution images, off-the-shelf low-resolution datasets such as LIP and Pascal-Person-Part are welcome adopted for pre-training. This new track poses a new task of learning from imperfect data, transferring the learned knowledge from low-resolution images to high-resolution images.
                        </p>
                        <ul>
                            Details will be available soon.
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Rules -->
            <div class="col-lg-12" id="rules">

                <div class="section-title">
                    <h1>RULES</h1>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p> This year, we have two strict rules for all competitors.
                        </p>
                        <ol>
                            <li> For training, only the images provided in the training set are permitted.
                                Competitors can use the classification models pre-trained on the training set of
                                ILSVRC CLS-LOC to initialize the parameters but <strong
                                        class="text-danger">CANNOT</strong> leverage any datasets with
                                pixel-level annotations.
                                In particular, for Track 1 and Track 3, only the image-level annotations of 
                                training images can be leveraged for supervision and the bounding-box annotations 
                                are <strong class="text-danger">NOT</strong> permitted.
                            </li>
                            <li>We encourage competitors to design elegant and effective models competing for all
                                the tracks rather than ensembling multiple models.
                                Therefore, we restrict the parameter size of the inference model(s) should be <strong
                                        class="text-danger">LESS
                                    than 150M</strong> (slightly more than two DeepLab V3+ [10] models using Resnet 101 as the
                                backbone). The
                                competitors ranked at Top-3 are required to submit the inference code for
                                verification.
                            </li>
                        </ol>
                    </div>
                </div>
        </div>

        <!-- References -->
        <div class="col-lg-12" id="reference">

            <div class="section-title">
                <h2>References</h2>
            </div>
            <div class="trend-entry d-flex">
                <div class="trend-contents">
                <p>[1] George Papandreou, Liang-Chieh Chen, Kevin Murphy, and Alan L Yuille. Weakly-and semi-supervised learning of a dcnn for semantic image segmentation. In ICCV, 2015.</p>
                <p>[2] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, and Thomas S Huang. Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation. In CVPR, 2018.</p>
                <p>[3] Peng-Tao Jiang, Qibin Hou, Yang Cao, Ming-Ming Cheng, Yunchao Wei, and Hong-Kai Xiong. Integral object mining via online attention accumulation. In ICCV, 2019.</p>
                <p>[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: a large-scale hierarchical image database. In CVPR, 2009.</p>
                <p>[5] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.</p>
                <p>[6] Rui Qian, Yunchao Wei, Honghui Shi, Jiachen Li, Jiaying Liu, and Thomas Huang. Weakly Supervised Scene Parsing with Point-based Distance Metric Learning. In AAAI, 2019</p>
                <p>[7] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.  Learning Deep Features for Discriminative Localization. In IEEE CVPR, 2016.</p>
                <p>[8] Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, and Thomas Huang. Adversarial complementary learning for weakly supervised object localization. In IEEE CVPR, 2018.</p>
                <p>[9] Xiaolin Zhang, Yunchao Wei, Guoliang Kang, Yi Yang, and Thomas Huang. Self-produced guidance for weakly-supervised object localization. In ECCV, 2018.</p>
                <p>[10] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.</p>

                </div>
            </div>
            </div>
        </div>



    </div>

    <div class="col-lg-12" id="challenge2">
        <div style="display:inline-block;width:500px;">
            <script async="async" src="//rc.rev
            olvermaps.com/0/0/7.js?i=2hlmeh3dic1&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;br=19&amp;sx=0"
                    type="text/javascript"></script>
        </div>
    </div>
</div>
</div>
<!-- END section -->


<div class="footer">
    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="copyright">
                    <p>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                        Copyright &copy;<script>document.write(new Date().getFullYear());</script>
                        All rights reserved | This template is made with <i aria-hidden="true"
                                                                            class="icon-heart text-danger"></i> by
                        <a href="https://colorlib.com" target="_blank">Colorlib</a>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>


</div>
<!-- .site-wrap -->


<!-- loader -->
<div class="show fullscreen" id="loader">
    <svg class="circular" height="48px" width="48px">
        <circle class="path-bg" cx="24" cy="24" fill="none" r="22" stroke="#eeeeee" stroke-width="4"/>
        <circle class="path" cx="24" cy="24" fill="none" r="22" stroke="#ff5e15" stroke-miterlimit="10"
                stroke-width="4"/>
    </svg>
</div>

<script src="js/jquery-3.3.1.min.js"></script>
<script src="js/jquery-migrate-3.0.1.min.js"></script>
<script src="js/jquery-ui.js"></script>
<script src="js/popper.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/owl.carousel.min.js"></script>
<script src="js/jquery.stellar.min.js"></script>
<script src="js/jquery.countdown.min.js"></script>
<script src="js/bootstrap-datepicker.min.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/aos.js"></script>
<script src="js/jquery.fancybox.min.js"></script>
<script src="js/jquery.sticky.js"></script>
<script src="js/jquery.mb.YTPlayer.min.js"></script>


<script src="js/main.js"></script>

</body>

</html>